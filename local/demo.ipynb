{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6888d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import base64\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85903197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Literal\n",
    "\n",
    "# --- Pydantic Schema Definitions ---\n",
    "\n",
    "# Schema for a single anomaly detected\n",
    "class Anomaly(BaseModel):\n",
    "    modality: Literal['image', 'audio', 'sensor', 'overall'] = Field(description=\"The source of the anomaly.\")\n",
    "    description: str = Field(description=\"A brief description of the detected anomaly.\")\n",
    "    severity: Literal['Low', 'Medium', 'High', 'Critical'] = Field(description=\"The severity level of the anomaly.\")\n",
    "\n",
    "# Schema for the complete comparison response\n",
    "class ComparisonReport(BaseModel):\n",
    "    triage_Level: str = Field(description=\"The model generates the level of triage\")\n",
    "    triage_action: str = Field(description=\"The model generates the action to undertake\")\n",
    "    reasoning_summary: str = Field(description=\"The model generates the summary reason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05839ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are the ALZHEIMER\\'S TRIAGE MONITORING AGENT (ATMA). Your sole purpose is to analyze multimodal inputs (image, voice transcript, and sensor data) to determine the immediacy of a life-threatening crisis for a dementia or Alzheimer\\'s patient.\\nYour response MUST be a final, decisive action classified into one of three Triage Levels. Do not engage in conversational chat or explain the diagnosis unless explicitly asked.\\n-- TRIAGE PROTOCOL --\\nINPUT ANALYSIS: You will receive the following:\\nIMAGE: A visual assessment (e.g., patient location, posture, visible injury).\\nVOICE TRANSCRIPT: A transcription of the patient or a nearby sound (e.g., patient vocalizations, distress, environmental sounds).\\nSENSOR DATA: A short text summary of objective vitals or environmental data (e.g., Heart Rate, Temperature, Door Status).\\n\\nCRITICAL REASONING: Use the following rules, prioritizing the most severe evidence:\\n\\nCRISIS (Level 1): Trigger if any of the following combinations are present:\\nEvidence of Fall (Image: Patient on floor, abnormal posture) AND Evidence of Acute Distress (Voice: Screaming, sudden crying, slurring, or groaning).\\nImage analysis confirms Visible Blood or Head Trauma AND Sensor data shows Elevated Heart Rate (>120 BPM).\\nSensor Data indicates Wandering/Hazard (e.g., \"Front door open at 3 AM\") AND Voice transcript contains Paranoid or Panicked Speech.\\nURGENT (Level 2): Trigger if evidence suggests significant risk but not immediate death:\\nImage shows Agitation/Pacing (e.g., restless movement) AND Voice transcript shows Severe Confusion or Verbal Aggression.\\nSensor data shows Fever (>101°F) AND Patient cannot articulate distress in voice transcript.\\nROUTINE (Level 3): Trigger for non-acute, but important, behavioral logging:\\nAny input suggests mild confusion, routine anxiety, or an environmental change that is not immediately life-threatening.\\n\\nIn the triage action choose one of three options EMERGENCY | IMMEDIATE | ROUTINE. DO NOT ADD ANYTHING ELSE.\\n\\n\\n-- OUTPUT FORMAT --\\nYour output must be a single JSON object.\\ncode\\nJSON\\n{\\n\"triage_tevel\": \"[Level 1 | Level 2 | Level 3]\",\\n\"triage_action\": \"[EMERGENCY | IMMEDIATE | ROUTINE]\",\\n\"reasoning_summary\": \"[Brief, 1-2 sentence explanation of why this level was chosen, citing the specific multimodal evidence from SENSOR DATA in 50 words NO MORE]\"\\n}'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\n",
    "\n",
    "with open(\"system_prompt.txt\", \"r\") as f:\n",
    "    SYSTEM_PROMPT = f.read()\n",
    "\n",
    "SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d45e296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Configuration ---\n",
    "# BASELINE SET\n",
    "# B_IMAGE_PATH = \"../data/baseline/Gemini_Generated_Image_f5zwnnf5zwnnf5zw.png\"\n",
    "B_IMAGE_PATH = \"./compressed_data/Gemini_Generated_Image_f5zwnnf5zwnnf5zw_compressed.jpg\"\n",
    "B_AUDIO_PATH = \"../data/baseline/-28U1_qW0sU_out.wav\"\n",
    "B_SENSOR_PATH = \"../data/baseline/sensor_data.json\" \n",
    "\n",
    "\n",
    "# Anomaly set\n",
    "# A_IMAGE_PATH = \"../data/min_anomaly/Gemini_Generated_Image_wnooknwnooknwnoo.png\"\n",
    "A_IMAGE_PATH = \"./compressed_data/Gemini_Generated_Image_wnooknwnooknwnoo_compressed.jpg\"\n",
    "A_AUDIO_PATH = \"../data/min_anomaly/-8Ezw9946g8_out.wav\"\n",
    "A_SENSOR_PATH = \"../data/min_anomaly/sensor_data.json\" \n",
    "\n",
    "# AA_IMAGE_PATH = \"../data/max_anomaly/Gemini_Generated_Image_gubff8gubff8gubf.png\"\n",
    "AA_IMAGE_PATH = \"./compressed_data/Gemini_Generated_Image_gubff8gubff8gubf_compressed.jpg\"\n",
    "AA_AUDIO_PATH = \"../data/max_anomaly/-4pUrlMafww_out.wav\"\n",
    "AA_SENSOR_PATH = \"../data/max_anomaly/sensor_data.json\" \n",
    "\n",
    "\n",
    "\n",
    "# Choose your deployed omnimodal model\n",
    "MODEL_NAME = \"gemma3:12b\"\n",
    "MODEL_NAME = \"gemma3:4b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c79f4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function: File Encoding ---\n",
    "def file_to_base64(file_path: str) -> str:\n",
    "    # ... (function defined previously - encodes file to base64 string) ...\n",
    "    # This function should be included in your final script\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: File not found at {file_path}\")\n",
    "    print(f\"Encoding file: {file_path}...\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        base64_string = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        print(f\"File {os.path.basename(file_path)} encoded. Length: {len(base64_string)}\")\n",
    "        return base64_string\n",
    "\n",
    "def load_sensor_data(file_path: str) -> str:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: File not found at {filepath}\")\n",
    "    print(f\"Encoding file: {file_path}...\")\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d083e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding file: ../data/baseline/sensor_data.json...\n",
      "Encoding file: ./compressed_data/Gemini_Generated_Image_f5zwnnf5zwnnf5zw_compressed.jpg...\n",
      "File Gemini_Generated_Image_f5zwnnf5zwnnf5zw_compressed.jpg encoded. Length: 152140\n",
      "Encoding file: ../data/baseline/-28U1_qW0sU_out.wav...\n",
      "File -28U1_qW0sU_out.wav encoded. Length: 2352104\n",
      "Encoding file: ../data/max_anomaly/sensor_data.json...\n",
      "Encoding file: ./compressed_data/Gemini_Generated_Image_gubff8gubff8gubf_compressed.jpg...\n",
      "File Gemini_Generated_Image_gubff8gubff8gubf_compressed.jpg encoded. Length: 156968\n",
      "Encoding file: ../data/max_anomaly/-4pUrlMafww_out.wav...\n",
      "File -4pUrlMafww_out.wav encoded. Length: 1163192\n",
      "\n",
      "Sending simple request to Ollama using model: gemma3:4b...\n",
      "\n",
      "--- Model Response ---\n",
      "```json\n",
      "{\n",
      "\"triage_tevel\": \"Level 1\",\n",
      "\"triage_action\": \"EMERGENCY\",\n",
      "\"reasoning_summary\": \"The real-time image shows the patient lying on the floor with an elevated heart rate of 125 BPM and low oxygen saturation (88%). This combination of evidence indicates a critical fall event requiring immediate attention and intervention.\"\n",
      "}\n",
      "```\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "                Given is the baseline image and audio.  \n",
    "                Given is the sensor data {json.dumps(load_sensor_data(B_SENSOR_PATH))}.\n",
    "                Understand the base line and use it as a reference for comparison.\n",
    "                \"\"\",\n",
    "            \"images\": [file_to_base64(B_IMAGE_PATH)],  # Only the single Base64 string here\n",
    "            \"audio\": [file_to_base64(B_AUDIO_PATH)]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "                Given is the real time image and audio. \n",
    "                Given is the sensor data {json.dumps(load_sensor_data(AA_SENSOR_PATH))}.\n",
    "                Compare with the baseline previously provided and generate the appropriate response as mentioned.\n",
    "            \"\"\",\n",
    "            \"images\": [file_to_base64(AA_IMAGE_PATH)],  # Only the single Base64 string here\n",
    "            \"audio\": [file_to_base64(AA_AUDIO_PATH)]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "print(f\"\\nSending simple request to Ollama using model: {MODEL_NAME}...\")\n",
    "response = ollama.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    options={'temperature': 0.7, \"num_threads\": 10}\n",
    ")\n",
    "\n",
    "# 4. Print the model's response\n",
    "print(\"\\n--- Model Response ---\")\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee3d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a1300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0c2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "739ae312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      "```python\n",
      "a, b = 0, 1\n",
      "while a <= 100:\n",
      "  print(a, end=\" \")\n",
      "  a, b = b, a + b\n",
      "```\n",
      "This code generates the Fibonacci sequence up to 100 using iterative approach, printing each number separated by a space.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_ollama_request(prompt_content):\n",
    "    \"\"\"Sends a prompt to the local Ollama API and prints the response.\"\"\"\n",
    "    \n",
    "    # The default local address for the Ollama service\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    # Data payload for the API request\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:12b\",  # Make sure you have this model pulled (ollama pull llama3)\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt_content}\n",
    "        ],\n",
    "        \"stream\": False  # Set to False to get the full response at once\n",
    "    }\n",
    "    \n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(url, data=json.dumps(payload))\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract the assistant's message content\n",
    "        assistant_message = result['message']['content']\n",
    "        print(f\"Model Response:\\n{assistant_message}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure Ollama is running and the model (llama3) is installed.\")\n",
    "\n",
    "# --- Run the function ---\n",
    "send_ollama_request(\"write a python code for fibonacci series uptil 100, please respond in 50 words or less. NO MORE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacdef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590b45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7fe83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224ca0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import subprocess\n",
    "from typing import Optional\n",
    "\n",
    "def compress_media_file(input_path: str, output_dir: str = \"./compressed_data\", quality: int = 80) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Compresses an image, audio, or video file to reduce size.\n",
    "    Returns the path to the compressed file, or None if compression fails.\n",
    "\n",
    "    :param input_path: Path to the original media file.\n",
    "    :param output_dir: Directory to save the compressed file.\n",
    "    :param quality: Compression quality (1-100, typically 80 is good).\n",
    "    :return: Path to the compressed file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Error: Input file not found at {input_path}\")\n",
    "        return None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name, file_ext = os.path.splitext(os.path.basename(input_path))\n",
    "    \n",
    "    # 1. Image Compression (Highest Impact: Convert PNG/BMP to JPEG)\n",
    "    if file_ext.lower() in ['.png', '.bmp', '.jpeg', '.jpg']:\n",
    "        output_path = os.path.join(output_dir, f\"{file_name}_compressed.jpg\")\n",
    "        try:\n",
    "            img = Image.open(input_path)\n",
    "            # Force conversion to JPEG format for maximum compression (lossy)\n",
    "            if img.mode == 'RGBA':\n",
    "                img = img.convert('RGB')\n",
    "            img.save(output_path, 'jpeg', quality=quality)\n",
    "            print(f\"Compressed image saved to: {output_path}\")\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error compressing image {input_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # 2. Audio/Video Compression (Requires FFmpeg)\n",
    "    # This uses a conceptual framework requiring the FFmpeg tool installed on the system PATH.\n",
    "    elif file_ext.lower() in ['.mp3', '.wav', '.mp4', '.mov']:\n",
    "        output_ext = '.mp3' if file_ext.lower() in ['.mp3', '.wav'] else '.mp4'\n",
    "        output_path = os.path.join(output_dir, f\"{file_name}_compressed{output_ext}\")\n",
    "        \n",
    "        # FFmpeg command for re-encoding audio/video to lower bitrate\n",
    "        if output_ext == '.mp3':\n",
    "            # Example: Convert to 64kbps MP3 (very small)\n",
    "            cmd = ['ffmpeg', '-i', input_path, '-b:a', '64k', output_path, '-y']\n",
    "        else: # .mp4 video\n",
    "            # Example: Re-encode video to crf 28 (higher compression, lower quality)\n",
    "            cmd = ['ffmpeg', '-i', input_path, '-vcodec', 'libx264', '-crf', '28', '-preset', 'veryfast', output_path, '-y']\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Compressed media saved to: {output_path}\")\n",
    "            return output_path\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: FFmpeg not found. Please ensure it is installed and in your system PATH.\")\n",
    "            return None\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error compressing media {input_path}: {e.stderr.decode()}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"Format {file_ext} not supported for internal compression. Returning original path.\")\n",
    "        return input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "804fc2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed image saved to: ./compressed_data/Gemini_Generated_Image_f5zwnnf5zwnnf5zw_compressed.jpg\n",
      "Compressed image saved to: ./compressed_data/Gemini_Generated_Image_wnooknwnooknwnoo_compressed.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./compressed_data/Gemini_Generated_Image_wnooknwnooknwnoo_compressed.jpg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_media_file(B_IMAGE_PATH)\n",
    "compress_media_file(A_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a1edbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed image saved to: ./compressed_data/Gemini_Generated_Image_gubff8gubff8gubf_compressed.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./compressed_data/Gemini_Generated_Image_gubff8gubff8gubf_compressed.jpg'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_media_file(AA_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595bcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
